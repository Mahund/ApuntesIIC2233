{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<font size='5' face='Georgia, Arial'>IIC-2233 Apunte Programación Avanzada</font><br>\n",
    "<font size='1'>&copy; 2015 Karim Pichara - Christian Pieringer. Todos los derechos reservados.</font>\n",
    "<br>\n",
    "<font size='1'> Modificado en 2018-1, 2018-2 por Equipo Docente IIC2233</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Testing_ \n",
    "\n",
    "Muchos programadores concuerdan con que el _testing_ es uno de los aspectos más importantes en el desarrollo de software. \n",
    "_Testing_ es el arte de generar código capaz de poner a prueba nuestros programas, chequeando que nuestro desarrollo pasa todas\n",
    "las pruebas en las cuales será sometido por los usuarios finales.\n",
    "En general, nosotros realizamos pruebas manuales cada\n",
    "vez que desarrollamos un nuevo pedazo de código que realiza alguna tarea. Sin embargo, es muy probable que nuestro testeo manual haya sido\n",
    "probado con un caso bastante común, lo cual no asegura que nuestro nuevo pedazo de código funciona en todos los posibles casos.\n",
    "Otro factor importante a considerar es el tiempo que invertimos realizando testeos manuales. Automatizar el testeo y generar un programa que se encargue de probar el sistema es una forma mucho más eficiente de asegurar la calidad del software que estamos construyendo.\n",
    " \n",
    "Desde ahora, todos los programas que ustedes realizarán en sus vidas deben ir de la mano de un programa que lo testea. No olviden que desde ahora, cualquier programa que desarrollen que no esté testeado significa que tiene _bugs_ (*“untested code is broken code”*).\n",
    "  \n",
    "En este capítulo veremos los conceptos básicos de _testing_ y cómo armar pruebas unitarias. Hay que tener claro que _testing_ es un capítulo que da para un curso completo. De hecho, existe un curso de _testing_ en el DCC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruebas unitarias\n",
    "\n",
    "Las pruebas unitarios (o _tests_ unitarios) se enfocan en _verificar_ unidades mínimas de código, donde cada _test_ está encargado de poner a prueba sólo una unidad simple del total de código que conforma el programa. Dentro de las librerías más usadas en Python para elaboración de _tests_ unitarios se encuentran: **pytest** y **unittest**. Ambas nos facilitan la vida a la hora de crear nuestros propias pruebas. Sin pérdida de generalidad, en este curso nos centraremos en **unittest**.\n",
    "\n",
    "\n",
    "### unittest\n",
    "\n",
    "El módulo `unittest` de Python provee muchas herramientas para crear y correr tests, una de las clases más importantes es `TestCase`. La clases creadas para testear deben heredar de la clase `TestCase`. Por convención todos los métodos que implementamos para testear **deben** comenzar su nombre con la palabra _test_, de tal forma de que sean reconocidos a la hora de correr el programa de testing en forma automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class ChequearNumeros(unittest.TestCase):\n",
    "    # este test debería funcionar\n",
    "    def test_int_float(self):\n",
    "        self.assertEqual(1, 1.0)\n",
    " \n",
    "    # este test debería fallar    \n",
    "    def test_str_float(self):\n",
    "        self.assertEqual(1, \"1\")\n",
    "        \n",
    "# Si quisiéramos ejecutar el test por consola:\n",
    "# if __name__ == \"__main__\":\n",
    "#     unittest.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".F\n",
      "======================================================================\n",
      "FAIL: test_str_float (__main__.ChequearNumeros)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-1-129a728b4e96>\", line 10, in test_str_float\n",
      "    self.assertEqual(1, \"1\")\n",
      "AssertionError: 1 != '1'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.004s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=1>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para ejecutar los tests aquí en el notebook:\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(ChequearNumeros)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto antes de la `F` indica que el primer test (`test_int_float`) pasó con éxito. Luego, la `F` después del punto indica que el segundo test falló. Después aparecen los detalles del _test_ que falló. Finalmente, podemos ver el número de _tests_ que se ejecutaron, el tiempo que tomó y el número total de _tests_ que fallaron.\n",
    "\n",
    "Podríamos tener entonces todos los _tests_ que necesitemos dentro de la clase que hereda de `unittest.TestCase`, siempre y cuando el nombre del método empiece con `test`. Cada _test_ debe ser completamente independiente de los otros, pues el resultado del cálculo de un _test_ no debe afectar el resultado de algún otro _test_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métodos de aserción \n",
    "\n",
    "En general, en los casos de test asignamos valores a ciertas variables, luego ejecutamos el código que queremos testear y finalmente nos aseguramos de que el resultado coincida con el valor correcto. Los métodos de aserción nos permiten validar los resultados de distintas formas, algunos métodos de aserción (incluidos en la clase `TestCase`) son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MostrarAsserts(unittest.TestCase):\n",
    "    \n",
    "    def test_aserciones(self):\n",
    "        a = 2\n",
    "        b = a\n",
    "        c = 1. + 1.\n",
    "            self.assertEqual([1, 2, 3], [1, 2, 3]) # falla si a != b\n",
    "            self.assertNotEqual(\"hola\", \"chao\") # falla si a == b\n",
    "            self.assertTrue(\"Hola\" == \"Hola\") # falla si bool(x) es False\n",
    "            self.assertFalse(\"Hola\" == \"Chao\") # falla si bool(x) es True\n",
    "            self.assertIs(a, b) # falla si a no es b\n",
    "            self.assertIsNot(a, c) # falla si a es b.\n",
    "            # Notar que \"is\" implica igualdad (==), pero no al revés,\n",
    "            # dos objetos distintos pueden tener el mismo valor.\n",
    "\n",
    "            self.assertIsNone(None) # falla si x no es None\n",
    "            self.assertIsNotNone(2) # falla si x es None\n",
    "            self.assertIn(2, [2, 3, 4]) # falla si a no está en b\n",
    "            self.assertNotIn(1, [2, 3, 4]) # falla si a está en b\n",
    "            self.assertIsInstance(\"Hola\", str) # falla si isinstance(a, b) es False\n",
    "            self.assertNotIsInstance(\"1\", int) # falla si isinstance(a, b) es True\n",
    "    \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(MostrarAsserts)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `assertRaises` acepta una excepción, algún _callable_ (función o cualquier objeto con el método `__call__` implementado) y un número arbitrario de argumentos (_keyworded_ o no) para ser pasados al método _callable_. La aserción va a invocar al método _callable_ con los argumentos y fallará si el método no genera el error esperado. El siguiente código muestra dos formas de cómo usar el método `assertRaises`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average(seq):\n",
    "    return sum(seq) / len(seq)\n",
    "\n",
    "class TestAverage(unittest.TestCase):\n",
    "    def test_python30_zero(self):\n",
    "        self.assertRaises(ZeroDivisionError, average, [])\n",
    "        \n",
    "    def test_python31_zero(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            average([])\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestAverage)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El administrador de contexto (_context manager_) nos permite escribir nuestro código de una forma más natural, llamando a la función directamente `average([])` en vez de tener que llamarla en forma indirecta a través de otra función `self.assertRaises(ZeroDivisionError, average, [])`.\n",
    "\n",
    "En Python 3.4 aparecieron nuevos métodos de aserción:\n",
    "\n",
    "``` python\n",
    "assertAlmostEqual(first, second, places=7, msg=None, delta=None)\n",
    "assertNotAlmostEqual(first, second, places=7, msg=None, delta=None)\n",
    "```\n",
    "\n",
    "Testean que `first` y `second` sean aproximadamente (o no aproximadamente) iguales, calculando la diferencia, redondeando al número dado de decimales (por defecto son 7). Si se provee el argumento `delta` en vez de `places`, la diferencia entre `first` y `second` debe ser menor o igual (o mayor en el caso de `assertNotAlmostEqual`) que `delta`. Si se provee `delta` y `places` se genera un error. Otras aserciones que existen son:\n",
    "\n",
    "\n",
    "``` python\n",
    "assertGreater(first, second, msg=None) # msg es el mensaje que se generará en la aserción\n",
    "assertGreaterEqual(first, second, msg=None)\n",
    "assertLess(first, second, msg=None)\n",
    "assertLessEqual(first, second, msg=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método `setUp`\n",
    "\n",
    "Una vez que hemos escrito varios _tests_, nos damos cuenta de que necesitamos armar un grupo de objetos que serán usados como input para comparar los resultados de un _test_. Además, para algunos _tests_, lo más probable es que estas variables deban ser modificadas. El método `setUp` nos permite declarar las variables que serán usadas para los _tests_. Este método se ejecuta antes de cada una de las pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class ListaEstadisticas(list):\n",
    "    def media(self):\n",
    "        return sum(self) / len(self)\n",
    "    \n",
    "    def mediana(self):\n",
    "        if len(self) % 2:\n",
    "            return self[int(len(self) / 2)]\n",
    "        else:\n",
    "            idx = int(len(self) / 2)\n",
    "            return (self[idx] + self[idx-1]) / 2\n",
    "        \n",
    "    def moda(self):\n",
    "        freqs = defaultdict(int)\n",
    "        for item in self:\n",
    "            freqs[item] += 1\n",
    "        moda_freq = max(freqs.values())\n",
    "        modas = []\n",
    "        for item, value in freqs.items():\n",
    "            if value == moda_freq:\n",
    "                modas.append(item)\n",
    "        return modas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 3, 4]\n",
      "[1, 2, 2, 3, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.014s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "class TestearEstadisticas(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.stats = ListaEstadisticas([1, 2, 2, 3, 3, 4])\n",
    "        \n",
    "    def test_media(self):\n",
    "        print(self.stats)\n",
    "        self.assertEqual(self.stats.media(), 2.5)\n",
    "        \n",
    "    def test_mediana(self):\n",
    "        self.assertEqual(self.stats.mediana(), 2.5)\n",
    "        self.stats.append(4)\n",
    "        self.assertEqual(self.stats.mediana(), 3)\n",
    "        \n",
    "    def test_moda(self):\n",
    "        print(self.stats)\n",
    "        self.assertEqual(self.stats.moda(), [2, 3])\n",
    "        self.stats.remove(2)\n",
    "        self.assertEqual(self.stats.moda(), [3])\n",
    "                \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestearEstadisticas)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `setUp` nunca es llamado explícitamente adentro de alguno de los tests, ya que unittest lo hace por nosotros.  Además podemos ver que en `test_mediana` se modificó la lista agregando un 4, pero luego en `test_moda` la lista es la misma que al principio. Esto ocurre porque `setUp` se encarga de volver a inicializar las variables necesarias al comienzo de cada _test_. Esto nos ayuda bastante a no repetir código innecesariamente. \n",
    "\n",
    "Además del método `setUp`, `TestCase` nos ofrece el método `tearDown`, que puede ser usado para _limpiar_ después de que se terminó de ejecutar el _test_. Por ejemplo, si el _test_ tiene la necesidad de crear algunos archivos, estos deberían eliminarse del sistema para no generar problemas con la ejecución del programa. Esto asegurará que el sistema estará en el mismo estado en que estaba antes de ejecutar los _tests_.\n",
    "\n",
    "Por ejemplo, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribiendo archivo temporal...\n",
      "Hola\n",
      "Eliminando archivos temporales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class TestearArchivo(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        self.archivo = open(\"prueba.txt\", \"w\")\n",
    "        self.diccionario = {1 : \"Hola\", 2 : \"Chao\"}\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.archivo.close()\n",
    "        print(\"Eliminando archivos temporales...\")\n",
    "        os.remove(\"prueba.txt\")\n",
    "        \n",
    "    def test_str(self):\n",
    "        print(\"Escribiendo archivo temporal...\")\n",
    "        self.archivo.write(self.diccionario[1])\n",
    "        self.archivo.close()\n",
    "        self.archivo = open(\"prueba.txt\", \"r\")\n",
    "        d = self.archivo.readlines()[0]\n",
    "        print(d)\n",
    "        self.assertEqual(self.diccionario[1], d)\n",
    "                        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestearArchivo)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizar los tests\n",
    "\n",
    "Cuando testeamos un programa, rápidamente nos empezamos a llenar de código únicamente de testeo. Para solucionar este problema, podemos organizar nuestros módulos que contienen _tests_ (objetos `TestCase`) en módulos más generales llamados _test suites_ (objetos `TestSuite`), que contienen colecciones de _tests_.\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestAritmetico(unittest.TestCase):\n",
    "\n",
    "    def test_arit(self):\n",
    "        self.assertEqual(1+1,2)\n",
    "\n",
    "class TestAritmetico2(unittest.TestCase):\n",
    "\n",
    "    def test_arit2(self):\n",
    "        self.assertNotEqual(2*1,1)\n",
    "        \n",
    "Tsuite = unittest.TestSuite()\n",
    "Tsuite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestAritmetico))\n",
    "Tsuite.addTest(unittest.TestLoader().loadTestsFromTestCase(TestAritmetico2))\n",
    "unittest.TextTestRunner().run(Tsuite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo ignorar tests fallidos\n",
    "\n",
    "Muchas veces sabemos que algunos de los _tests_ fallarán en nuestro programa. Por ejemplo, si tenemos una función que no está terminada, o si estamos corriendo una versión distinta de Python, o quizá en alguna plataforma específica, sabemos que el _test_ fallará. En estos casos, podríamos querer que no se reporte la falla o que el _test_ no se ejecute. Afortunadamente, Python nos provee de algunos elementos para marcar pruebas que sabemos que van a fallar o para que los ignore bajo ciertas condiciones. Estos elementos son: `expectedFailure`, `skip(reason)`, `skipIf(condition, reason)`, `skipUnless(condition, reason)`.\n",
    "\n",
    "Veamos algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sFsFx\n",
      "======================================================================\n",
      "FAIL: test_ignorar_if (__main__.IgnorarTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-206b99a8dea7>\", line 20, in test_ignorar_if\n",
      "    self.assertEqual(False, True)\n",
      "AssertionError: False != True\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_que_no_sabiamos_que_falla (__main__.IgnorarTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-9-206b99a8dea7>\", line 8, in test_que_no_sabiamos_que_falla\n",
      "    self.assertEqual(False, True)\n",
      "AssertionError: False != True\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.009s\n",
      "\n",
      "FAILED (failures=2, skipped=2, expected failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=5 errors=0 failures=2>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import sys\n",
    "\n",
    "class IgnorarTests(unittest.TestCase):\n",
    "    \n",
    "    # Este test fallará\n",
    "    def test_que_no_sabiamos_que_falla(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "    @unittest.expectedFailure\n",
    "    def test_sabemos_que_falla(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "    @unittest.skip(\"Test inútil\")\n",
    "    def test_ignorar(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "    @unittest.skipIf(sys.version_info.minor < 5, \"No funciona en Python 3.1.\")\n",
    "    def test_ignorar_if(self):\n",
    "        self.assertEqual(False, True)\n",
    "        \n",
    "    @unittest.skipUnless(sys.platform.startswith(\"linux\"), \"No funciona, a excepción de Linux.\")\n",
    "    def test_ignorar_unless(self):\n",
    "        self.assertEqual(False, True)\n",
    "    \n",
    "\n",
    "                        \n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(IgnorarTests)\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La `x` en la primera línea significa “falla esperada” (_expected failure_).\n",
    "\n",
    "La `s` significa “test ignorado” (_skipped test_).\n",
    "\n",
    "La `F` significa “falla real” (_real failure_). Los resultados están en orden alfabético."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
